URGENT IMPORTANT
The money and rare skills are somewhere between software and data engineering. 
Knowing the in/outs of some distributed processing system (spark, flink, dataflow), 
various distributed database systems (cassandra, redshift, ddb, etc), some 
pipeline orchestrater (airflow, luigi etc) and some pub sub system(kafka / pubsub).


OBJECTIVE
 -given an interview question, we should be able to speak out minds in a coherent manner with proper clarity
 -we are able to deduce an algorithm and we are able to solve the problem
 -we have a decent portfolio of projects





ALGORITHM
   -Start by watching mock interviews(till NLP specializtion)
    -get the words or the phrases most commonly used(which we feel adds value) 
    -practice these phrases


   -For every question(3-4 hours)
    -speak your ideas
    -write a mock(if possible)
    -see the solution
    -explain the concept and answer(first time)
    -write the entire answer while parallely explaining the ideas

REQUIRED
Required skills and experience

Bachelors/Masters or higher degree in Computer Science or related technical field. 
  5+ years of experience as Machine Learning Engineer with expertise in full stack development.
  Good understanding of Machine learning and Deep learning techniques.
  Apply machine learning, data science techniques and design/develop distributed systems.
  Expertise in development of web services/APIs for serving model results.
  Experience with containerization (Docker) and container-orchestration systems such as Kubernetes.
  Proficient in MLOps with Kubeflow/MLFlow and comfortable in deploying, monitoring and managing large scale models in production.
  Good knowledge of data structures, algorithms and excellent problem-solving skills in Python.
  Familiar with frameworks like PyTorch/TensorFlow/Keras or similar.
  Good experience with Big Data frameworks like Hadoop, Spark, Pig ,Hive, Flink, MapReduce etc.
  Good experience in handling high-volume streaming data/events.
  Excellent communication, analytical and evaluative thinking skills.


####################
RESUMES

1. TCS - Oracle - Amazon - Some_startup - Google

Examples of linkedin profile
Software Development Engineer
Company NameAmazon Internship
Dates EmployedMay 2017 – Jul 2017
Employment Duration3 mos
LocationHyderabad Area, India
• Developed a Customer Service Action of Cancelling Purchase Item for Amazon scale.
• Designed an efficient Algorithm and developed Contracts on Amazon based UI using different APIs for the action.
• Developed Plugin using Java as the programming language and simulate all the Contracts and Plugin.


Software Development Engineer
Company NameCodeNation Full-time
Dates EmployedAug 2018 – Nov 2019
Employment Duration1 yr 4 mos
< CLOUDCRM >
• Developed a platform Cloud CRM to provide open data model for CRMs. It has an impact to save 100 million USD per year
• Developed a serverless AWS lambda for bi-directional sync between Jira and Cloud CRM.
• Used Debezium for change data capture, AWS Kinesis to collect and process real time streaming data.
• Exposure to technologies like AWS Lambda, DynamoDB, AWS Kinesis, AWS S3, Python(language) etc.

< COST ANALYZER >
• Developed a product CostAnalyzer to prevent Out Of Memory Exception in production.
• Statically analyzed program byte code to get heap memory consumption information during its entire lifetime.
• Responsible for designing the architecture, models, as well as the algorithms required for the analysis.
• Exposure to technologies like Spring MVC, Neo4j, AWS S3, Docker, Graph Database, Java (language) etc.


Software Engineer 2
Company NameSumo Logic Full-time
Dates EmployedNov 2019 – Jan 2021
Employment Duration1 yr 3 mos
<Health Tracker>
• Working on the scalability and reliability aspect of a product that tracks the health of all existing assemblies
• Scaled the product to 10x, now having the capacity to process 1 million requests per second
• Used Kafka in distributed systems environment to receive incoming events in-order in real-time.
• Exposure to technologies like Distributed Systems, EC2 instances, Apache Kafka, DynamoDB, Scala(language) etc



2. NIT - Adobe
Currently managing and leading the development of product suite for building a user-centric, "single Experience Cloud" in which our customers access composable capabilities to enhance development velocity.

• Trident - A TypeScript based library that is responsible for automatizing the design and implementation of an adaptive, accessible, and robust SPA as per the input schema.

- Pioneered the architecture and development of Trident framework and responsible for the elicitation of new requirements and incremental implementation of UX components.
- Working on tech stack Reactjs along with Typescript, Webpack, and Babel
- Leading and mentoring a team of 5 members in architectural and design decisions.

• SPA Pipeline - Single Stop for building all Single Page Applications

- Led product vision, roadmap, and design of highly optimized, and reliable CI/CD Pipeline for Single Page Applications from scratch for teams across Adobe Experience Cloud in collaboration with Product Owners and Stakeholders; 80% increase in the development velocity of developers and a huge saving on infrastructure costs.
- Onboarded 28 teams to the Pipeline, over a span of 14 months, with out-of-the-box support for capabilities such as security analysis, bundle size check, accessibility analysis, internationalization, etc. Introduced various features in Pipeline for optimizing the build and runtime-related performance of Single Page Applications leading to less load times.
- Expanded my knowledge in CI/CD, Jenkins, Groovy, Azure Blob store, Grafana Dashboarding for monitoring services

• Unified Onboarding

- Managing Jenkins Pipeline that does the setup and configuration-related tasks to onboard the SPA into unified shell, SPA pipeline and GraphQl with zero effort from developers.


3. PhonePe?
• Working on an open source project Fission: Serverless Functions for Kubernetes
• Introduced event driven scaling in Fission by integrating KEDA: Kubernetes Event-driven Autoscaling with fission’s
message queue trigger making smooth adaption fission in an event driven architecture.
• Introduced concurrency in poolmanager strategy using which one can specialize pods concurrently to serve multiple
requests.
• Developed KEDA connectors for Apache Kafka and RabbitMQ used in fission’s KEDA based message queue trigger.
• Actively working to improve scalability under heavy load for long running functions and on reducing cold start time.
• Migrated fission CI pipeline from Travis CI to Github Actions.
• Worked on improving RBAC authorization for fission.
Crest Data Systems Ahmedabad, India
SOFTWARE ENGINEER May 2017– April 2020
• Experience on working end to end projects, including requirements gathering, solution design and development.
• Created reusable, testable, and efficient code using best programming practices.
• Developed best-practices and mentored junior developers and interns.
• Reviewed code, debugged problems and corrected issues
• Experience in unit testing, load testing, Test-Driven Development (TDD) and Continuous Integration.
• Participating in the investigation and troubleshooting of application issues or failures as well as recommending necessary software modifications or enhancements as per design specifications.
Crest Data Systems Ahmedabad, India
SOFTWARE ENGINEER INTERN Jan 2017– Apr 2017
• Worked on development of Application Performance Management (APM) Module for Splunk IT Service Intelligen


4. IIT - BCG- Bain 
 Associate
Company NameBain Capital
Dates EmployedJun 2018 – Jul 2020
Employment Duration2 yrs 2 mos
LocationMumbai Area, India
> Evaluated 25+ investment opportunities, primarily across tech services vertical, with some industrials & healthcare exposure
> Led financial modelling, engaged with advisors & mgmt. teams to shape view on business sustainability & potential returns
> Invested in Brillio, a fast-growing digital tech services company: owned the financial modelling effort, along with work on macro growth, valuation multiples, margins & deal closure
> Drove portfolio management on Brillio post-investment: tracking operational & financial performance, assessing M&A opportunities, working closely with CFO to refine revenue forecasting & cost mgmt. measures, while ensuring balance sheet health, etc.


The Boston Consulting Group (BCG)
Associate
Company NameThe Boston Consulting Group (BCG)
Dates EmployedSep 2016 – Apr 2018
Employment Duration1 yr 8 mos
LocationMumbai Area, India
Oil & Gas - Retail transformation project for India's largest commercial enterprise. Leading the implementation of several key initiatives across multiple geographies & markets.
Healthcare - Strategy design for drug R&D outsourcing. Led the design process for multiple verticals, servicing different stages of the R&D industry.


5. LIDO - Flipkart

LIDO
Company NameLIDO
Total Duration2 yrs 1 mo
Title
Software Development Engineer - II

Employment Duration1 yr
LocationMumbai, Maharashtra, India
• Working as Backend-Lead in the organization and implemented major back-end and front-end features
including schema designing for different dashboards and integrated them to improve overall user
experience.
• Developed Live Streaming and Recording System, Student Leaderboard Service etc.
• Built JWT Authentication system with refresh mechanism to enhance the security.
see less
Title

Software Development Engineer - I
Full-time
Dates EmployedAug 2019 – Aug 2020
Employment Duration1 yr 1 mo
LocationMumbai, India
• Implemented GraphQl APIs to fetch client-oriented data instead of server-oriented.
• Developed two layer time-based caching for GraphQL APIs using Redis.
• Implemented PubSub for live streaming of data using Ably.
• Have worked on Scaling servers using ECS with zero downtime while deployment.
• Implemented CI/CD pipeline using Jenkins for hassle free deployment.
• Have experience of working on AWS Services like S3, Cloudfront, Lambda, ECS, ELB etc.
• Setup CDC on PostgreSQL using Debezium and Kafka.

Leveraged Knowledge: GraphQL, NodeJS, ReactJS, Python, PostgreSQL, AWS, Hasura, Apollo,
Docker, Jenkins, Redis, Ably, Alibaba Cloud, HLS, Debezium, Kafka

6. NSIT - Samsung
About
AI Computer Vision expert/enthusiast and avid researcher , Strong engineering professional 
graduated from Netaji Subhas Institute of Technology with 8.46 GPA, Woking as 
Senior Machine Learning Engineer in Samsung Bangalore

Having extensive experience in developing high accuracy commercialisation purpose end to end 
machine learning On Device solutions/pipelines . Experience in handling and developing tools 
for maintaining , analysis , streamlining of millions of images and data .

Skilled in data science, Machine Learning , Android Development , deep learning, 
Tensorflow, model optimization and profiling , Python , Object Oriented Programming , 
data structure algorithm , System Design

Samsung India
Total Duration2 yrs 3 mos
TitleSenior Software Engineer
Full-time
Dates EmployedMar 2021 – Present
Employment Duration6 mos
-> Working on providing deep learning solutions for videos. Developed solutions to process Videos On device through machine learning models with less power consumption and high accuracy.
-> Developing optimised deep learning solutions according to commercialization project/User requirements.
-> Mentoring College students as part of Samsung Prism program
-> Worked extensively on Quality Aware Template Matching deep learning solutions
TitleSoftware Engineer

Full-time
Dates EmployedJun 2019 – Mar 2021
Employment Duration1 yr 10 mos
LocationBangalore
-> Worked on developing and on end to end on device deployment of state of the art commercialized Machine Learning models.
-> Developed artificial intelligence pipelines to provide commercialization solutions ( Content Moderation ALTZ project commercialized in Samsung A51 and A71)
-> Developed extensive tools to handle, analyse , streamline lakhs of images and high intensity of data.
-> Worked on optimising and adding new features to Samsung gallery application
see less

Samsung India
Software Development Intern
Company NameSamsung India
Dates EmployedMay 2018 – Jul 2018
Employment Duration3 mos
LocationBengaluru Area, India
-> Worked In the Communication and Networking Department of Samsung R&D.
-> Made enode B login tool to automatically collect the logs
-> running the teraterm script and collect logs


Universities
- University of Texas, Dallas
- New York University
- University of California, San Diego


7. Cure Fit - Google
Software Engineer
Company NameGoogle Full-time
Dates EmployedAug 2020 – Present
Employment Duration1 yr 1 mo
LocationBengaluru, Karnataka, India
- Built a system to plan and automate PnL attributions for Google Ads by creating a data pipeline, consuming data from different financial and monitoring systems. This helped improve transparency and precision for PnL costs. Was involved in system design, UI design, working over product requirements.
- Worked on an upcoming Area 120 project to implement Web Share APIs to allow bringing more users to the platform.
- Languages and Frameworks used: C++, Java, Typescript, Angular, internal Web Frameworks.

CultGear
Software Engineering Intern
Company NameCultGear Internship
Dates EmployedSep 2019 – Mar 2020
Employment Duration7 mos
LocationBengaluru Area, India
• Worked on a tool to manage the designs under development and incorporate customer feedback in the phase to reduce monetary losses. Worked on both Frontend and Backend.
• Working on the Data Ingestion Layer of an upcoming smart device to create user profiles and provide relevant insights based upon their health metrics. Designed architecture to withstand data coming continuously from million+ devices.


Cure.Fit
Software Engineering Intern
Company NameCure.Fit Internship
Dates EmployedSep 2019 – Mar 2020
Employment Duration7 mos
LocationBengaluru Area, India
• Worked on a tool to manage the designs under development and incorporate customer feedback in the phase to reduce monetary losses. Worked on both Frontend and Backend.
• Working on the Data Ingestion Layer of an upcoming smart device to create user profiles and provide relevant insights based upon their health metrics. Designed architecture to withstand data coming continuously from million+ devices.


Shipsy
Software Engineering Intern
Company NameShipsy Internship
Dates EmployedMay 2019 – Jul 2019
Employment Duration3 mos
LocationGurgaon, India
• Used Apache Airflow to transform ETL pipeline with real-time monitoring, reducing failure rates by 50%.
• Built a system to scrape couriers’ tracking websites and provide the clients with status monitoring for their packages on a unified dashboard.
• Built a POC of an AR Application that measured parcel dimensions without user interaction.


Shipsy
Software Development and Data Science Intern
Company NameShipsy Internship
Dates EmployedFeb 2018 – Jul 2018
Employment Duration6 mos
LocationGurgaon, India
• Built an end to end Machine Learning pipeline to recognize text from handwritten postal labels with an accuracy of 71%.
• Reduced frame processing time by 3 times on Shipsy’s Rider Android App by implementing custom CameraView and using Facebook’s Fresco for reducing memory consumption.
• Used Firebase ML Kit to implement Barcode Scanning. Doubled the rate of scan by making efficient use of multithreading.


8. IBM - VMware - Oracle - Amazon - Microsoft

Software Engineer - II
Company NameMicrosoft Full-time
Dates EmployedSep 2019 – Present
Employment Duration2 yrs 1 mo
LocationBengaluru Area, India
Microsoft 365 Outlook Calendar Platform

o Set and achieved SLA availability of 99.9 for all the Calendar APIs.
o Set process for prioritizing Customer reported issues/escalations.
o Working on modernizing existing APIs into micro-services architecture.
Amazon
Software Development Engineer - II
Company NameAmazon Full-time
Dates EmployedNov 2018 – Aug 2019
Employment Duration10 mos
LocationBangaon Area, India
• Part of Amazon Spark(https://amazon.com/spark) - Content Ranking Engine team.
• Involved in all phases of development from requirement discussion with stakeholders till deployment of the code to prod.
• Involved in designing(HLD, LLD), implementing, unit testing, integration testing, deploying features for Spark.

Technologies : Java 8, Hive, Lombok, AWS EMR, AWS S3, AWS EC2, AWS Elastic Search, Amazon DynamoDB, Amazon Redshift, Amazon Kinesis, Amazon SQS
see less
Oracle
Senior Member Of Technical Staff
Company NameOracle
Dates EmployedJun 2017 – Nov 2018
Employment Duration1 yr 6 mos
LocationBangaon Area, India
• Member of Oracle CASB (Cloud Access Security Broker)
• Contributing to Data ingestion framework for ingesting audit data from cloud applications.
• Contributing to Cloud Access Simulator, which simulates the cloud data that are being used by CASB.
• Working for integrating multiple cloud application to Oracle CASB.
• Contributing to multiple modules of Oracle CASB like Policy engine, Reports, Threat alert.
• Designing new components, reviewing new designs, coding different modules of CASB.
• Working on AWS SDK APIs and Box APIs for Data Ingestion.
• Writing Junit test cases for the newly developed features.

Technologies : Java, Maven, Cassandra, Hadoop, Kafka, AWS API

Domain : Cloud Security
…
see more
Hewlett Packard Enterprise
Software Engineer - II
Company NameHewlett Packard Enterprise
Dates EmployedMar 2016 – Jun 2017
Employment Duration1 yr 4 mos
LocationBangalore
• Member of HPE OneView Development team in HPE R&D Lab.
• Working for Cluster Resource Manager (CLRM) component of OneView.
• Designing new components, reviewing new designs.
• Written new REST APIs to manage Clusters, Hosts, and Infrastructure VMs.
• Working on VMware Hypervisor layer by making VI SDK call to VMware Infrastructure.
• Writing Junit test cases for the newly developed features.

Domain : Hyper-converged Infrastructure, Integrated Infrastructure
…
see more
VMware R&D
Member of Technical Staff - II
Company NameVMware R&D
Dates EmployedSep 2013 – Mar 2016
Employment Duration2 yrs 7 mos
LocationBengaluru Area, India
• Member of VMware vCloud Director development team for Continuous Product Development (CPD) group.
• Working on Placement Engine and Storage Profiles module of VMare vCloud Director.
• Implementing new feature with optimized algorithm.
• Analysing, triaging and fixing the Customer bugs with the most efficient algorithm.

Domain : Cloud Computing, Virtualization, IaaS
see less
IBM India Software Labs
Software Engineer
Company NameIBM India Software Labs
Dates EmployedFeb 2011 – Sep 2013
Employment Duration2 yrs 8 mos
LocationBangalore
• Member of VMControl development team for IBM Systems Director product.
• Experience in developing/ enhancing new modules of VM Control CLI/ REST APIs.
• Involved in Unit Testing and Debugging in Eclipse.
• Experience in Java Plugin developement.
• Experience in REST API automation for VMControl.

Domain : Data Center Virtualization and Cloud Computing
see less


9. Software Engineer
Flipkart - Headout - Google - Facebook
Company Name
Facebook Full-time
Dates EmployedSep 2021 – Present
Employment Duration1 mo
LocationLondon, England, United Kingdom

Google
Software Engineer II - Search Ads
Company NameGoogle Full-time
Dates EmployedAug 2019 – Sep 2021
Employment Duration2 yrs 2 mos
Headout
Software Engineer - Backend
Company NameHeadout Full-time
Dates EmployedOct 2018 – Aug 2019
Employment Duration11 mos
LocationBengaluru Area, India
• Created a end to end automated chatbot system which helps customer’s query
about their booking.
• Created a login microservice that allows users to login to the website using
google and facebook accounts.
• Created an offline internal tool that generates booking tickets.
• Arranged coding interviews and contests for campus recrutiment .
• Technologies : Python, AWS, TkInter, Docker, Kotlin, Spring, Django, MariaDB.


Flipkart
Data Engineer
Company NameFlipkart Internship
Dates EmployedJan 2018 – Jun 2018
Employment Duration6 mos
Studied and provided userinsights using the raw data and pushing it into
Flipkart Data Platform.
• Wrote a spark application that can check the parity of insights using Resilient
Distributed Datasets
• Scheduled hodoop workflow jobs using azkaban to write data to HIVE.
• Technologies : Hadoop, MapReduce, Spark, Kafka, HIVE, Python


Ittiam Systems Pvt Ltd
Machine Learning Intern
Company NameIttiam Systems Pvt Ltd Internship
Dates EmployedMay 2017 – Jul 2017
Employment Duration3 mos
LocationBengaluru Area, India
Built a prototype on NVIDIA GPU’s using deep learning to perform multi-object
classification on live video.
• Studied and analyzed deep learning methods and frameworks.
• Achieved real-time object classification on videos and images
• Technologies : Tensorflow, Python

10. Capgemini
    Software Engineer Mar 2021 — Present
    Capgemini Pune
    • Integrating the project functions and resources across the product life cycle, right from planning building, testing, and deployment
    to support. Performance monitoring for Application servers using Prometheus.
    • Creating and maintaining of docker environment for microservice applications using Dockerfile and docker compose.
    • Creating and maintaining AWS EC2, S3 and VPC services


11. Interviewbit - Linkedin - Google

    Software Development Engineer
Company NameLinkedIn Full-time
Dates EmployedJul 2020 – Jun 2021
Employment Duration1 yr
LocationBengaluru, Karnataka, India
• Contributing to the LinkedIn Enterprise Provisioning Application based on Java(Rest.Li) and EmberJS.
• Working on the LSS(LinkedIn Sales Service) to UPP(Unified Provisioning Platform) migration including features like preordering, system and business logic failure handling, provision monitoring and recovery life cycle, and Grant of LSS products via CAP(Corp Account Portal).
• Worked on LLS(LinkedIn Learning Service) service with provisioning workflow coupled with SFDC and Dynamics with features including handling of use cases like cancellation before grant, adding new REST.LI endpoints for accounts with Lynda migration status and case creation failure alerts.
• Worked on Job Slot product's admin REST.Li APIs for changing the amount granted and left allocations which reduced the on-call Job Slot debugging time from 3 days to less than an hour.
• Contributed to the CSP-Web which is the customer support tool for the new provisioning flow with multiple frontend features.


CodeChef
Competitive Programming Educator
Company NameCodeChef
Dates EmployedOct 2020
Employment Duration1 mo
Educated students in competitive programming. Discussed topics like C++ STL, Difference Vectors, Policy-Based DS, Recursion, Dynamic Programming, Number Theory, C++ Fundamentals, etc.
InterviewBit
Company NameInterviewBit
Total Duration11 mos
TitleSoftware Engineer
Full-time
Dates EmployedFeb 2020 – Jun 2020
Employment Duration5 mos
LocationBengaluru, Karnataka, India
• Worked on the Ruby on Rails and React-based web application of InterviewBit (Scaler) Academy.
• Worked in a React-based generic chat module for the Scaler website. Used MobX for state management and implemented features like message sending and reception, editing and deleting a message, sharing different media, etc.


TitleSoftware Engineer
Internship
Dates EmployedAug 2019 – Feb 2020
Employment Duration7 mos
LocationBengaluru Area, India


Berkman Klein Center for Internet & Society at Harvard University
Google Summer Of Code'19 @ Berkman Klein Centre For Internet And Society at Harvard University
Company NameBerkman Klein Center for Internet & Society at Harvard University Internship
Dates EmployedMay 2019 – Aug 2019
Employment Duration4 mos
LocationIndia
• Worked on the Question Tool application based on MeteorJS.
• Implemented CRUD features of Questions and instances.
• Built the PDF archive download feature for Questions to summarize the discussion of the questions.
• Build the highlight feature on the question instances so that admin can control the highlighting of the question remotely and
it will show the highlight on the user's end as well. Used Websocket based integration using Streamy module of MeteorJS
• Link to the implemented features: https://summerofcode.withgoogle.com/projects/#5809707464261632
• Link to the proposal: https://drive.google.com/file/d/1tG33LLWhy2lApiXaPl1ukRv7xho0V-Ne/view?usp=sharing
…
see more


Coding Blocks
Product Engineer
Company NameCoding Blocks Internship
Dates EmployedMay 2018 – Aug 2019
Employment Duration1 yr 4 mos
LocationNew Delhi Area, India
• Worked on Blocks Forms, A ruby on rails based generalized
data collection and form framework
• Mentoring students at Coding Blocks for Advanced algorithms in C++, Java, And Competitive Programming. Reviewing and Developing educational content for coding students including the development of a centralized questionnaire database.
see less


kakcho
Full Stack Engineer
Company Namekakcho Internship
Dates EmployedJun 2018 – Aug 2018
Employment Duration3 mos
LocationNew Delhi Area, India
• Created the backend in Ruby On Rails, Node.JS, & Socket.io & maintained AWS server with EC2 & S3 instances & troubleshooting 99% server issues.
• Identified software & DB architecture based on project requirements, performing version control &, refining product visual design of 4 screens.
• Scaled complete backend with horizontal scaling to increase efficiency by 60%.
see less


Fable HQ
Software Engineer
Company NameFable HQ Internship
Dates EmployedJan 2018 – Feb 2018
Employment Duration2 mos
LocationNew Delhi Area, India
• Developed user interface module for Creative Selector module in Angular 5 & gathered requirements, develop and test module to meet requirements.
• Revamped use cases, API & user stories of the complete web app.


12. Data Science - Hackathons

Ericsson
Total Duration3 yrs
TitleSenior Software Engineer
Full-time
Dates EmployedAug 2020 – Present
Employment Duration1 yr 2 mos
LocationBengaluru, Karnataka, India
Working with the R&D team to design and develop new usecases.
Develop Prototypes and POCs for the new features
TitleSoftware Engineer
Full-time
Dates EmployedOct 2018 – Jul 2020
Employment Duration1 yr 10 mos
LocationBengaluru, Karnataka, India
Worked on various ML projects.
Worked on Production deployment of ML projects.
Responsible for developing POCs for ML usecases.
IBM
Oracle Application Developer
Company NameIBM Full-time
Dates EmployedFeb 2016 – Nov 2017
Employment Duration1 yr 10 mos
LocationPune, Maharashtra, India


13. TCS - MS - Facebook
Leading Data Engineer with 3+ years of experience in working on a petabyte size data platform built using cloud and big data technologies for supporting machine learning experiments, data science models, business intelligence reporting and data exchange with internal and external partners.

Programming Languages: Python, SQL, Java, Scala
Databases: Hive, MS SQL Server, MySQL, Oracle 11g
Big data Ecosystem: Apache Spark, HDFS, Databricks, Delta Lake, Sqoop, Impala, Hue, Apache Kafka
Cloud Stack: Azure Data Factory, Azure Data Lake Store, HDInsight
ETL and BI Skills: Data Profiling, Data Integration, Data Modelling, Talend, Alteryx, Power BI, Tableau

Happy to chat about any topics from data related world

Data Engineer
Company Name
Facebook Full-time
Dates EmployedSep 2021 – Present
Employment Duration2 mos
 

Northeastern University
Business Intelligence Specialist
Company NameNortheastern University Part-time
Dates EmployedJan 2021 – Jul 2021
Employment Duration7 mos
LocationBoston, Massachusetts, United States
Working as a part of the Academic Technologies Team, where we support all software related technologies in the universities

As BI Developer, my responsibilities are to build visualizations in PowerBI for Deans, Directors, administrators of the university
see less


Retail Business Services
Data Engineering
Company NameRetail Business Services Full-time
Dates EmployedJun 2020 – Dec 2020
Employment Duration7 mos
LocationQuincy, Massachusetts, United States
Retail Business Services, LLC, is the services company of Ahold Delhaize USA, currently providing services to Five grocery brands Stop & Shop, Food Lion, The GIANT Company, Giant Food and Hannaford.

Member of Business Intelligence and Data Team which supports any request to and from centralized Azure Data Lake Store

Responsibilities:

-Building centralized data mart in Curated Data Layer using Azure data lake Store for Personalized Marketing Campaign

-Developing Spark application using Pyspark and Spark-SQL in Databricks to uncover insight about customer usage patterns

-Creating delta tables in Databricks and partitioned Hive tables with configurable data pipeline using Azure Data Factory
…
see more
Tata Consultancy Services
Company Name


Full-time
Dates EmployedMay 2018 – Oct 2019
Employment Duration1 yr 6 mos
LocationPune Area, India
- Member of Analytics and Insights group of TCS and Hadoop Developer in the data platform team for Leading British multinational insurance company
- Designed and Implemented reusable and extendable data ingestion framework to ingest data from various data sources onto Hadoop data lake
- Data from sources like Relational Databases, Flat Files was ingested by just changing the config file which reduced development time by 60%
- Collaborated with Data Scientist for data cleaning, validation, and transformation processes utilizing Spark DataFrames, data was further used by Data Scientist to build Machine learning models
- Involve in designing data models and writing Hive queries leveraging HiveQL for business stakeholders to analyze various metrics
- Optimized Sqoop jobs to decrease ingestion time from 12 hours to 8 hours while ingesting from the Oracle database
- Collaborated with the DevOps team, for creating ansible scripts to replicate the environment in Development, QA, and Production environment
- Scheduled the data pipelines using Control-M, provided support to the maintenance/support team for any issues or job failures
- Actively participated in scrum ceremonies with distributed agile teams across multiple continents
- Documented technical specifications for the project in Confluence in a way that it would be easy to understand and maintain the application



TitleJunior Data Engineer
Full-time
Dates EmployedMay 2016 – May 2018
Employment Duration2 yrs 1 mo
LocationIndia
Project: General Data Protection Project(GDPR) Compliance
- Ingested Personally identifiable information (PII) data from the various system in the organization to a common data platform - Cloudera Data Platform (CDP)
- Analyzed schema from every source system and integrated data from all sources to a common schema using Spark Dataframes
- Developed a Kafka Consumer Application to get GDPR request in JSON format
- Spark Application was developed to match a percentage of similarity of GDPR request to records in data lake and matching was found utilizing the spaCy library, the resulting percentage was submitted via HTTPS POST
- Implemented utilities to delete Personally identifiable information (PII) data of an individual from Avro, Parquet, CSV, XML files formats, and Hive
-Collaborated with the testing team, to build the smoke testing framework which was used to check for common functionalities when the application is deployed to QA and production environment, this led to decrease production bugs by 10%
see less


14.  BITS - Google Intern - Uber
https://www.linkedin.com/in/ravishankar-joshi/
Software Engineer
Uber Full-time
Dates EmployedAug 2020 – Present
Employment Duration1 yr 3 mos
LocationBengaluru, Karnataka, India
• Working in Core Data team on a front-end project.
• Tech Used: JavaScript, ReactJS, GraphQL

Google
Software Engineering Intern
Google Internship
Dates EmployedMay 2019 – Jul 2019
Employment Duration3 mos
LocationBengaluru Area, India
• Worked with GSuite team on a backend project, "On-call observatory" - a dashboard to display anomalies in the performance metrics of micro-services to help site reliability engineers solve outages faster.
• Successfully captured the root causes of a few previously known outages using the dashboard.
• Tech Used: Python.

Homi Bhabha Centre for Science Education (HBCSE), TIFR
Summer Intern
Company NameHomi Bhabha Centre for Science Education (HBCSE), TIFR Internship
Dates EmployedMay 2018 – Jul 2018
Employment Duration3 mos
LocationMumbai Area, India
• Worked on back-end development, on the e-learning website CLIx made by HBCSE.
• Implemented features for making courses downloadable as an epub (similar to PDF), a Moodle course and an offline website.
• These features would allow the students and teachers to access the course contents offline after downloading it once.
• Tech Used: Python, Django.


15. Shuttl - Maersk

Engineering Manager
A.P. Moller - Maersk Full-time
LocationCopenhagen, Capital Region, Denmark
Building an end to end Supply Chain Management platform.

Shuttl
Total Duration3 yrs 2 mos
Engineering Manager
Full-time
Dates EmployedSep 2019 – Jul 2021
Employment Duration1 yr 11 mos
LocationGurgaon, Haryana, India
- Managing a team of 15, which includes backend and mobile engineers. My team is responsible to develop solutions for consumer business - customer experience, growth, payments
- As an EM, my core responsibilities are to align with the product and do project management for my team. I am also responsible for people development in my team which includes career growth and appraisals.
- Some value delivered by my team includes:
— Built driver and customer gps pipeline.
— Built ETA and Tracking enhancements in Shuttl app which reduced complaints by 70%
— Inventory optimisation in the consumer business PAN India which reduced our operating costs by 20% on high occupancy route.
— New customers are allowed to book anytime unlike the opening time for regular customers which helped increase our conversion by 10%.
— Automated numerous marketing operations in payments, subscriptions and campaigns which helped reduce churn by 25% and hence increased our growth.
— Reduced service error rate to 20 errors/Million requests from 250 errors/Million requests.

TitleSenior Software Engineer
Dates EmployedApr 2019 – Sep 2019
Employment Duration6 mos
LocationGurgaon, India
- Automated CRM(Clevertap) integration at scale. Developed an event pipeline to sync over millions of events per day.
- Developed a compeletely instrumented webhook processor to do actions at scale via the CRM. The processor processes millions of actions required for growth at Shuttl
- Evangelised importance of instrumentation and documentation within the team. We extensively use Datadog for instrumentation and document all ideas for the team before developing them.

LocationGurgaon, Haryana, India
- Major contributor in the rewrite of a stressed monolith to numerous distributed services at scale.
- Developed the coupons engine, orders and payments services as a sole developer
- Built employee transport management system as a part of the enterprise tech team.


Kayako
Product Engineer
Company NameKayako
Dates EmployedJul 2016 – Jun 2018
Employment Duration2 yrs
LocationGurgaon, India
- Automated Sales pipeline for +10k customers. Integrated Salesforce with customer product lifecycle.
- Automated billing pipeline for $8M ARR business. Integrated system with Zuora (for subscription and billing management).
- Controlled first successful technical implementation of product tracking using ETL pipeline built over Segment, Autopilot, and Mixpanel for mapping user journey.
- Engineered infrastructure for automated product internationalization, increasing adoption across 30 languages.
- Co-hosted weekly customer interviews as part of the Growth team.
- Shipped core product features like Remember Me support, UI improvements, SLA improvements.
…

OWASP Foundation
Student Developer
Company NameOWASP Foundation
Dates EmployedApr 2016 – Aug 2016
Employment Duration5 mos
Developed a stand-alone tool for Offensive Web Testing Framework(OWTF) using python which converts raw HTTP request to scripts in desired language.


##################################################################################################


Skills required to become a data engineer

Industry Knowledge
Shell Scripting
Agile Methodologies
Extract, Transform, Load (ETL)

Tools & Technologies
Python (Programming Language)
Scala
SQL
Azure Databricks
Hadoop mapreduce
Azure Data Factory
Azure Data Lake
Microsoft Excel
Microsoft Word
Java

Microsoft Power BI
Tableau
Hadoop
MySQL
Microsoft PowerPoint
SQL DB2
Sqoop
Data Engineering
Talend Open Studio



###################################################################3
EXPECTATIONS
Figure this out
Once our web application running in the Prod environment failed everytime 
we had more than 1000 concurrent connections. The docker containers 
would mysteriously restart every few minutes. The logs wouldn't say much either.

On careful analysis of the thread dump we confirmed that the 
3rd party Logging framework we used would hang the system thread on 
doing multiple concurrent IO writes, leading to container restarts. 
Surprisingly this behaviour was not reproducible in the test environment.

And just like that, with no issue in our own code, we got a Tier 1 escalation. 
After spending a few hours on their Github issue logs we bumped up the Logging 
library version and the error disappeared.

When you use a third party library to solve a problem in your code, 
their problems now become your problem.



Hackerrank - backend engineer
Overall of 2+ years expertise in at least one dynamic programming language and one MVC web framework, preferably Ruby on Rails. 
Expertise in Database modelling, preferably relational DB, spotting DB bottlenecks and writing optimized DB queries.
Expertise in all common AWS Cloud technologies, Knowledge of CI/CD tools and workflows.
Proven track record in building highly-available and scalable distributed systems


Hackerrank - lead backend engineer
Overall of 6+ years expertise in at least one dynamic programming language and one MVC web framework, preferably Ruby on Rails.
Experience in building out large/key projects and products from scratch.
Expertise in Database modelling, preferably relational DB, spotting DB bottlenecks and writing optimized DB queries.
Expertise in all common AWS Cloud technologies, Knowledge of CI/CD tools and workflows.
Proven track record in building highly-available and scalable distributed systems.


Hackerrank - senior backend engineer
7+ years of backend development
Built RESTful backend infrastructure and API services using Ruby on Rails
Experience using databases like MySQL
You understand and have built CI/CD, NGINX, Docker, Kubernetes, CLI
Familiarity with AWS and front-end development tools like Webpack, Babel, NPM, Git, etc.


SUPPLY - DEMAND
(CHECK THIS OUT - THIS IS OUR LEARNING PATH)
1. Hackerrank - senior data engineer
You will be working on:
Design, Build and Maintain real-time and batch ETL pipeline that can scale
Architect, Develop and maintain our data warehouse and analytics database
Collaborate on critical technology decisions concerning architecture and toolset
Production machine learning models by exposing APIs
Take ownership of scaling, performance, security, and reliability of our data infrastructure
Establish, mentor and advice on data engineering best practices


We are looking for:
3+ years of experience with designing, developing, and maintaining robust Python ETL Pipelines
3+ years of experience with database technologies - MySQL, Postgres, AWS Aurora, Redshift, etc.
Experience with Data Streaming Architecture
Experience with the Hadoop ecosystem - PySpark, AWS EMR, etc.
Experience with Python coding language
Experience with Apache Airflow.
Experience querying massive datasets using Spark, Presto, etc.
Experience with SQL performance tuning.
Deployed ML models in a production environment
You have built business intelligence applications
Able to solve problems of scale, performance, security, and reliability


Nice to have:
Modern Microservices based architectures with Serverless systems
Git and deployment tools
Machine Learning
Web Backend and API development
DevOps and Server Management on AWS or GCP
Experience working on Kafka



Software Architect
Overall of 9+ years expertise in software engineering and architecture.
Proven experience with building scalable, fault-tolerant and highly-available systems.
Experience in building out large/key projects and products from scratch.
Expertise in Database modelling, preferably relational DB, spotting DB bottlenecks and writing optimized DB queries.
Expertise in all common AWS Cloud technologies, Knowledge of CI/CD tools and workflows.
Knowledge of containerization and orchestration systems (k8s) is a highly-preferred.


2. Dream 11
- Machine Learning Engineer
Your Role:

Scaling Data Science model for serving 100M+ users.
Scaling Data Science development lifecycle, Enabling model training and inferencing at scale.
Designing robust, scalable platform for enabling Data Science personalised model.
Scaling M/L algorithms, processing huge dataset with distributed processing systems like Apache Spark/Apache Flink.
Work with the data engineering team to design effective solutions for ETL pipelines and Near real-time data aggregations.
Leading cross-functional initiatives and collaborating with engineers across teams.

Must Have:

3+ years experience as Machine Learning Engineer or related field
Experience in building batch & streaming applications on platforms like Apache Spark/Apache Flink/KSQL.
Experience with structuring machine learning systems for production
Hands-on experience in designing and debugging distributed solutions with NoSQL/In-memory/data-grid tools like (Cassandra/Apache Ignite/Aerospike/HBase/Apache Ignite/Scylladb/Voltdb)
Thoroughly versed with distributed technologies, strategies for building low latency, high throughput real-time systems are highly scalable.

Good to Have:

Experience in building ML ops infrastructure for ML teams to train and test ML models using tools like kubeflow/databricks/sagemaker
Experience with feature engineering at scale, i.e. experience with workflow management platforms like airflow/prefect/step/Jenkins or equivalent ones
Understand Linear Algebra / Numerical Computing as in Gilbert Strang's book
Experience in applied data analysis, including A/B testing
Experience with large-scale distributed data processing systems, cloud infrastructure such as AWS, Azure or GCP, and container systems such as Docker
Familiarity with running Spark, Dask jobs
Completed CS 329S: Machine Learning Systems Design course

- SDE Platform

Understanding and solving real business needs at a large scale by applying your distributed systems engineering and analytical problem-solving skills
Designing & building solutions using distributed processing frameworks like AKKA/Apache Spark/Apache Flink for realtime product/platform use cases
Designing optimized and scalable solutions that use In-memory data grid and distributed NoSQL store (Cassandra, Apache Ignite, Aerospike)
Architecting and building a robust, scalable, and highly available solutions for use cases like A/B testing, personalizations, scaling M/L algorithms
Working with the data engineering team to design effective solutions for ETL pipelines and near realtime data aggregations
Leading cross-functional initiatives and collaborating with engineers across teams
Must Have:
3+ years of hands-on experience in applying and building distributed systems
Experience with building and debugging an application with NoSQL/In-memory data grid-like (Cassandra/Apache Ignite/Aerospike/HBase/Apache Ignite)
Experience in building batch & streaming applications on platforms like Apache Spark/Apache Flink/KSQL.
Experience in building services/platform components with java frameworks like Vert.x
Good to Have:
Experience with building reactive systems with a toolkit like Vertx and AKKA
Experience in designing & handling big data solutions in production
Experience working with the AWS/GCP stack
Prior experience with running services on Kubernetes clusters
Prior experience working with agile methodologies & CI/CD pipeline
Good communication skills

- SDE 2/3 - Core Infrastructure

Our Tech Team is the core of Dream11’s mobile-first cross-platform (Android & iOS, Mobile + Desktop PWA) product, serving more than 10 Crore users with over 70 million rpm (requests per minute) at peak with user concurrency of 5.5 million. Our tech stack is hosted on AWS and comprises multiple distributed systems like Cassandra, Aerospike, Akka, Voltdb, Ignite etc.

We have around 100+ micro-services primarily written in Java backed by vert.x framework. They serve isolated product features with discrete architectures to serve the respective use-cases. We have a completely in-house data infrastructure built on top of Kafka, Redshift, Spark, Druid etc. which powers our Machine Learning and Predictive Analytics use-cases. We ingress Terabytes of Data every day, which flows all over our Data pipelines to power a plethora of use-cases. 
Your Role:
Designing and implementing the services and tools we need to manage and scale a service oriented architecture, e.g. service discovery, config managers, container orchestration and more
Designing network and security Implementations at scale
Building cloud agnostic architectures with multi/hybrid cloud approaches
Delivering solid Infrastructure as a code by using automation tools such as Terraform/Ansible/Python
Building and designing self serve infrastructure automation on scale
Building and maintaining container orchestration platform
Optimising and improving deployment workflow
Developing and supporting tools for infrastructure to be used by other engineers
Evangelising best practices to the wider engineering organisation
Mentoring/supporting engineers regarding development, concepts and best practices
Must Have:
Extensive unix/linux & networking understanding
Strong understanding of the software development lifecycle
Experience coding in higher-level languages (e.g.,Python, Golang or Java)
Strong design patterns, system architecture, and best practices
Good to Have:
Strong experience working with multi cloud (AWS/GCP/AZURE), Docker and container orchestration technologies such as Kubernetes and Terraform
Good understanding of observatory tools and logging
Enthusiasm for open source contribution
Previous experience working in an environment with micro-services
Knowledge of IT security
Experience in distributed and big data technologies

- Lead Machine Learning Engineer

Orchestrating online experimentation at scale to provide insights in a standard format for consumption across the company
Designing and maintaining DAGs which move Machine Learning processes towards continuous experimentation and evaluation
Performing distributed machine learning training and designing distributed Machine Learning Engineering Pipelines
Working with platform engineering team in designing API contracts which are scalable for multiple years

Must Have:
5+ years of experience as a ML Engineer or Software Engineer or Data Engineer or related fields
Solid understanding of concepts such as model drift, model validation, continuous ML deployment
Very good understanding of Distributed ML training and deployment (preferably via Spark, Dask, EMR based setups)
Experience building modular ML systems that integrate with product
Good to Have:
Understanding concepts such as cost sensitive learning, tweaking of cost functions, deep learning concepts and implementations
Ability to implement SOTA ML pipelines/systems described by good papers end to end
Completed CS 329S: Machine Learning Systems Design course 

3. CashKaro.com

Send InMail
About CashKaro / EarnKaro:

CashKaro is India's No 1 Cashback platform and a leading marketing partner for Amazon, Flipkart, Myntra and 1000+ more sites. EarnKaro is India’s first social commerce platform, leveraging affiliate marketing in our mission to create 1 million entrepreneurs. We have recently closed a $10mm series B round.

  

The Role:

We are looking for an experienced Data Warehouse Specialist to build and manage our Data Warehouse. This is an individual contributor role, covering initial analysis, data cleaning, DW setup, deployment and maintenance.


Min 4 years in Data Warehousing with strong experience in AWS technologies like Redshift, Glue, Athena, S3 and EMR.
Proven experience in setting up Data Warehouses from scratch using a 2-tier or 3-tier architecture and/or rearchitecting Data Warehouses.
Experience in creating and maintaining ETL pipelines using AWS Glue, Lambda, Python, etc.
Experience in designing Data Warehouse Schemas for easy business reporting.
Experience in integrating different data sources like GA, Firebase, Google Ads, Facebook Ads, Netcore, Clevertap, Branch, Freshdesk, and other data sources commonly used in B2C Apps / Websites.
Designing and implementing Stored Procedures (SPs) and SQL scripts for migrating data to DataMart and for extraction
Experience in AWS Lake formation on S3 and redshift.
Expert level in SQL and proficient in Python / R.


4. SDE-2 Licious

We are currently searching for 4-8 years of developers responsible for development of our back-

end technology stack. The successful candidate will be a key player in the Engineering Team.

Required Skills/Qualifications:

● B.Tech or equivalent in Computer Science or equivalent with 4-8 years of experience in

building enterprise web applications.

● Experience in some or all of the following: Amazon Web Services (AWS), REST-based

Web Service APIs using either MEAN (Mongo, Express.js, Angular.js and Node.js) or

LAMP (Linux, Apache, MySQL and PHP) and other web-related technologies.
● Strong foundation in database schema design and development either in Relational
Datbases like MySQL or NoSQL Databases like MongoDB.
● Experience with UI technologies like JavaScript, CSS, HTML, Angular, Bootstrap, etc.
● Strong experience in version control software (GitHub, SVN, etc.), IDE tools (Eclipse)
and Unit testing (JUnit).
● Experience working with services in AWS such as EC2, RDS, and ELBs and have
knowledge of VPCs.
● Good hands on experience in system design database design is must
● Prior experience in leading small teams is a plus
● Ability to reason about performance trade-offs.
● Exposure to Continuous Integration (CI) and Continuous Deployment (CD), automated
testing and agile development methods.
● Exposure to Devops.
● Experience working with micro-services and distributed computing environments.
● Good knowledge on design patterns

Preferred Skills/Qualifications:

● Experience in building scalable systems.
● Exposure to building and using reusable code and libraries.
● Candidates should proactively keep their knowledge and skills up to date.
● Candidates should be able to work in a fast paced Agile Software Development
environment independently and as part of the team.
● Candidates should be able to work in a highly collaborative environment with excellent

communication skills.

Culture fitments:

● Candidate should be enthusiastic and looking for the next challenging stint in career
● Should be comfortable with startup work culture
● Ownership and accountability should be his/her DNA

5. Rockstar - Data Engineer
   RESPONSIBILITIES 
  Resolve operational issues as they occur to maintain the team’s SLAs.
  Implement and support big data tools and frameworks such as HDFS, Hive, and Impala.
  Implement and support data models using Spark and Spark-ML.
  Assist in the development of deployment automation and operational support strategies on Hadoop and Snowflake.
  Deliver near-real time and non-near-real-time data and applications to a team of analysts and data scientists who create insights and analytics applications for our stakeholders.
  Maintain and extend our CI/CD processes and documentation.
  QUALIFICATIONS
  5+ years of work experience with ETL, Data Modeling, and Business Intelligence Big Data Architectures.
  5+ years of experience with the Hadoop ecosystem (Map Reduce, Spark, Spark-ML, Oozie, Hive, Impala, etc.) and big data ecosystems (Kafka, Cassandra, etc.).
  Experience developing and managing data warehouses on a terabyte or petabyte scale.
  Experience developing Machine Learning pipelines and data models.
  Strong experience in massively parallel processing & columnar databases.
  Experience with Python and shell scripting.
  Experience working in a Linux environment.
  Deep understanding of advanced data warehousing concepts and track record of applying these concepts on the job.
  SKILLS
  Expert in at least one SQL language such as T-SQL or PL/SQL.
  Good communication skills.
  Dynamic team player.
  A passion for technology - we are looking for someone who is keen to leverage their existing skills and seek out new skills and solutions.
  PLUSES
  Please note that these are desirable skills and are not required to apply for the position.

  Experience in real-time analytics applications.
  Experience in Lambda architecture and On-Premise Clusters.
  Experience with Java or Scala programming languages.
  Experience with CI/CD.
  Knowledge of RestAPI and Artifactories.
  Knowledge of the video game industry

-1. Backend engineer - Twitter
   - Good at Distributed backend systems

#########################################################################
Leetcode - tech stack discussion
1. Dream 11
   Java(Springboot), javascript(node.js), AWS SQS, Postgres, Mongodb, Kakfa, mysql, 

2. Myntra
   In Myntra, My team primarily works on Spring boot, Kafka, HBase, Mongo, and Redis.



Motivation
- Always hire people who do 'X+ ∆X' work, where 'X' is the expectation 
  and '∆X' is the work they are doing because they want it to be great!
- You can see this in their past work, projects and personal hustle. 
  Look for what was the 'X' - the expectation and what was '∆X' in past work.


Linkedin
- Rework required in
  1. Licenses and certifications
     Courses on parallelism
  2. Open source contributions
  3. projects
  4. Detailed experience explanation


STUFF REQUIRED TO BECOME SDE-2
1. Algo, DS, 
2. System Design, HLD and LLD
3. Distributed systems
4. Good evidence of you developing some features to product/production


Mentorship guide

How will I help my mentees:

1. Prepare a roadmap

2. Regular mock Interviews

3. Resolving career related queries

4. Help in building effective Resume

5. Once the mentee is ready, then how to apply companies both Indian and International



A few years ago, I led a small team that mostly consisted of 
college freshers. On one occasion a junior engineer informed me 
that his code was getting a Memory leak issue while doing load 
testing with multiple concurrent threads. I sat beside him and 
showed him how to debug Memory leakage in a JVM runtime environment. 
The manager silently watched this entire episode and later brought 
me to a room and said conspicuously:


YOU CAN BECOME A BETTER SOFTWARE ENGINEER BY PRACTICING Programming,

GETTING A GOOD GITHUB PROFILE, having employee recommendations by impressing
current software employees working in top companies by impressing them on Github
